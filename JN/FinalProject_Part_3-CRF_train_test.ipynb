{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Part 3: CRF model train and test</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have ready-to-use data, vectorized and tagged.<br>\n",
    "We present training CRF model with several paramaters, and it's results.\n",
    "\n",
    "TBD: add tested parameter list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch vectorized data\n",
    "below we get the preprocessed data from project data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading vectorized data from: C:\\Users\\moshe\\Desktop\\CS_courses\\2018_2019\\NLP\\FinalProject\\repository\\data\\vectorized_data.txt\n",
      "vectorized data file read OK\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..\\GenerationPipe')\n",
    "import NLP_HEBPUNCT_GP_generator as gen\n",
    "\n",
    "# fetch vectorised data\n",
    "fileName = \"vectorized_data.txt\"\n",
    "scriptPath = os.path.dirname(os.path.realpath('__file__'))\n",
    "scriptPathPrev = os.path.split(scriptPath)[0]\n",
    "relFilePath_data = \"\\\\data\\\\\" + fileName\n",
    "dataFilePath = scriptPathPrev + relFilePath_data\n",
    "\n",
    "print(\"reading vectorized data from: %s\" % dataFilePath)\n",
    "if (os.path.isfile(dataFilePath)):\n",
    "    dataFile = open(dataFilePath, \"r+\", encoding='utf-8')\n",
    "    print(\"vectorized data file read OK\")\n",
    "    \n",
    "    data = eval(dataFile.read())\n",
    "    # print(data[0])\n",
    "else:\n",
    "    print(\"print: file not found in %s\" % dataFilePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data set size       :  7528\n",
      "training set size   :  6776\n",
      "test set size       :   752\n"
     ]
    }
   ],
   "source": [
    "trainRatio   = 0.9\n",
    "dataSetSize  = len(data)\n",
    "trainSetSize = math.ceil(dataSetSize*trainRatio)\n",
    "testSetSize  = dataSetSize - trainSetSize\n",
    "\n",
    "print(\"data set size       : %5d\" %dataSetSize)\n",
    "print(\"training set size   : %5d\" %trainSetSize)\n",
    "print(\"test set size       : %5d\" %testSetSize)\n",
    "\n",
    "train = data[:trainSetSize]\n",
    "test  = data[trainSetSize:]\n",
    "\n",
    "x_train = [x[0] for x in train]\n",
    "y_train = [x[1] for x in train]\n",
    "\n",
    "x_test = [x[0] for x in test]\n",
    "y_test = [x[1] for x in test]\n",
    "\n",
    "\n",
    "# print(\"-----------------------------------------------------\")\n",
    "# print(x_train[0])\n",
    "# print(\"-----------------------------------------------------\")\n",
    "# print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRF Suite - training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit done\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(x_train, y_train)\n",
    "\n",
    "print(\"fit done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD: remove\n",
    "# y_pred = crf.predict(x_test)\n",
    "# print(\"---------------------------------------------------\")\n",
    "# for i in range(len(y_test[0])):\n",
    "#     if (y_pred[0][i] != y_test[0][i]):        res = \"FAIL\"\n",
    "#     else:                                     res = \"OK\"\n",
    "#     print(\"%3d - truth: %4s prediction: %4s %4s\" %(i, y_test[0][i], y_pred[0][i], res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRF Suite - results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          ,       0.01      0.37      0.02       165\n",
      "          .       0.19      0.96      0.32       759\n",
      "\n",
      "avg / total       0.16      0.85      0.26       924\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = crf.predict(x_test)\n",
    "\n",
    "labels = list(crf.classes_)\n",
    "labels.remove('none')\n",
    "labels.remove('!')\n",
    "labels.remove('?')\n",
    "\n",
    "metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='weighted', labels=labels)\n",
    "print(metrics.flat_classification_report(y_pred, y_test, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRF Suite - hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moshe\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:14: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Mapping, namedtuple, Sized\n",
      "C:\\Users\\moshe\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\moshe\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  9.9min\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 34.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'c1': 0.2037329577368321, 'c2': 0.04593907323483032}\n",
      "best CV score: 0.14425496762360251\n",
      "model size: 0.33M\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted', labels=labels)\n",
    "\n",
    "# search\n",
    "rs = RandomizedSearchCV(crf, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        n_iter=50,\n",
    "                        scoring=f1_scorer)\n",
    "\n",
    "rs.fit(x_train, y_train)\n",
    "\n",
    "# crf = rs.best_estimator_\n",
    "print('best params:', rs.best_params_)\n",
    "print('best CV score:', rs.best_score_)\n",
    "print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X validation results:\n",
    "\n",
    "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  9.9min<br>\n",
    "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 34.3min finished<br>\n",
    "\n",
    "best params: {'c1': 0.2037329577368321, 'c2': 0.04593907323483032}<br>\n",
    "best CV score: 0.14425496762360251<br>\n",
    "model size: 0.33M<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.2037329577368321,\n",
    "    c2=0.04593907323483032,\n",
    "    max_iterations=200,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(x_train, y_train)\n",
    "\n",
    "print(\"fit done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = crf.predict(x_test)\n",
    "\n",
    "labels = list(crf.classes_)\n",
    "labels.remove('none')\n",
    "labels.remove('!')\n",
    "labels.remove('?')\n",
    "\n",
    "metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='weighted', labels=labels)\n",
    "print(metrics.flat_classification_report(y_pred, y_test, labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
