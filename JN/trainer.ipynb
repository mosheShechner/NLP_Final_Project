{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Data From Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block generates our corpus. That is -\n",
    "1. download text from Wikipedia pages\n",
    "2. creates clean text (with not punctuation) = X\n",
    "3. learn the correct labeling of each word = Y , where $y \\in Y$ is $y \\in $ {'None','Comma','dot'}\n",
    "\n",
    "After running this block the corpus = X will be in the file 'pos_to_tag.txt' in the sampe folder of this JN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "sys.path.append('..\\GenerationPipe')\n",
    "import NLP_HEBPUNCT_GP_generator as gen\n",
    "import NLP_HEBPUNCT_GP_pre_processing as preproc\n",
    "import subprocess\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding to data set URL: https://he.wikipedia.org/wiki/%D7%A7%D7%A4%D7%99%D7%98%D7%9C%D7%99%D7%96%D7%9D\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Administrator\\\\Documents\\\\University\\\\Semester_C\\\\NLP\\\\Project\\\\NLP_Final_Project\\\\NLP_Final_Project\\\\data\\\\train_data_indx.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-baacb86a741c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargetUrlList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-baacb86a741c>\u001b[0m in \u001b[0;36mgenerate_dataset\u001b[1;34m(url_list)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgenerate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"train_data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetStringFromDataFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train_data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"<DATADEL>\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\University\\Semester_C\\NLP\\Project\\NLP_Final_Project\\NLP_Final_Project\\GenerationPipe\\NLP_HEBPUNCT_GP_generator.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(URLList, outFileName, verbose)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mURL\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mURLList\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"adding to data set URL: %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mURL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0maddURLToData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mURL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutFilePath_indx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutFilePath_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetStringFromDataFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilePrefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\University\\Semester_C\\NLP\\Project\\NLP_Final_Project\\NLP_Final_Project\\GenerationPipe\\NLP_HEBPUNCT_GP_generator.py\u001b[0m in \u001b[0;36maddURLToData\u001b[1;34m(URL, outFilePath_indx, outFilePath_data)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;31m# add URL to index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0moutFile_indx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutFilePath_indx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0moutFile_indx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mURL\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0moutFile_indx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Administrator\\\\Documents\\\\University\\\\Semester_C\\\\NLP\\\\Project\\\\NLP_Final_Project\\\\NLP_Final_Project\\\\data\\\\train_data_indx.txt'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "targetUrlList = [\"https://he.wikipedia.org/wiki/%D7%A7%D7%A4%D7%99%D7%98%D7%9C%D7%99%D7%96%D7%9D\",\n",
    "                 \"https://he.wikipedia.org/wiki/%D7%A7%D7%A8%D7%9C_%D7%9E%D7%A8%D7%A7%D7%A1\",\n",
    "                 \"https://he.wikipedia.org/wiki/%D7%90%D7%99%D7%9C%D7%9F_%D7%A8%D7%9E%D7%95%D7%9F\",\n",
    "                 \"https://he.wikipedia.org/wiki/%D7%90%D7%9C%D7%91%D7%A8%D7%98_%D7%90%D7%99%D7%99%D7%A0%D7%A9%D7%98%D7%99%D7%99%D7%9F\",\n",
    "                 \"https://he.wikipedia.org/wiki/%D7%90%D7%94%D7%91%D7%94\",\n",
    "                 \"https://he.wikipedia.org/wiki/%D7%9E%D7%A6%D7%99%D7%90%D7%95%D7%AA\",\n",
    "                 \"https://he.wikipedia.org/wiki/%D7%A4%D7%99%D7%9C%D7%95%D7%A1%D7%95%D7%A4%D7%99%D7%94_%D7%A9%D7%9C_%D7%94%D7%9E%D7%93%D7%A2\",\n",
    "                 \"https://he.wikipedia.org/wiki/%D7%9B%D7%91%D7%99%D7%93%D7%94\",\n",
    "                 \"https://he.wikipedia.org/wiki/%D7%94%D7%9E%D7%90%D7%94_%D7%94-20\",\n",
    "                 \"https://he.wikipedia.org/wiki/%D7%9E%D7%96%D7%A8%D7%97_%D7%90%D7%99%D7%A8%D7%95%D7%A4%D7%94\",\n",
    "                 \"https://he.wikipedia.org/wiki/%D7%97%D7%95%D7%A7_%D7%94%D7%94%D7%A1%D7%93%D7%A8%D7%99%D7%9D\",\n",
    "                 \"https://he.wikipedia.org/wiki/%D7%94%D7%94%D7%A1%D7%AA%D7%93%D7%A8%D7%95%D7%AA_%D7%94%D7%9B%D7%9C%D7%9C%D7%99%D7%AA_%D7%A9%D7%9C_%D7%94%D7%A2%D7%95%D7%91%D7%93%D7%99%D7%9D_%D7%91%D7%90%D7%A8%D7%A5_%D7%99%D7%A9%D7%A8%D7%90%D7%9C\",\n",
    "                 \"https://he.wikipedia.org/wiki/%D7%90%D7%93%D7%A8%D7%99%D7%9B%D7%9C%D7%95%D7%AA_%D7%92%D7%95%D7%AA%D7%99%D7%AA\",\n",
    "                 \"https://he.wikipedia.org/wiki/%D7%9B%D7%A0%D7%A1%D7%99%D7%99%D7%AA_%D7%94%D7%91%D7%A9%D7%95%D7%A8%D7%94\",\n",
    "                 \"https://he.wikipedia.org/wiki/%D7%94%D7%9E%D7%97%D7%9C%D7%95%D7%A7%D7%AA_%D7%A2%D7%9C_%D7%94%D7%98%D7%A7%D7%A1%D7%99%D7%9D_%D7%94%D7%A1%D7%99%D7%A0%D7%99%D7%99%D7%9D\"]\n",
    "\n",
    "\n",
    "\n",
    "def word2features(word,i):\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word[:1]': word[:1],\n",
    "        'word[:2]': word[:2],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'index': i\n",
    "\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "def words2features(words_list):\n",
    "    vectors = []\n",
    "    for i in range(len(words_list)):\n",
    "        vectors.append(word2features(words_list[i],i))\n",
    "    return vectors\n",
    "\n",
    "def texts_to_files(lst):\n",
    "    file = open(\"words_to_pos.txt\",\"w\") \n",
    "    for x in lst:\n",
    "        s = x[:x.index(\"<XYDEL>\")]\n",
    "        file.write(s + \" \") \n",
    "    file.close() \n",
    "\n",
    "def generate_dataset(url_list):\n",
    "    gen.generate(url_list,\"train_data\")\n",
    "    out = gen.getStringFromDataFile(\"train_data\")\n",
    "    out = out.split(\"<DATADEL>\")\n",
    "    texts_to_files(out)\n",
    "    data = []\n",
    "    for x in out:\n",
    "        data.append(x.split(\"<XYDEL>\"))\n",
    "    dataset=[]\n",
    "    for d in data:\n",
    "        temp = []\n",
    "        temp.append(d[0].split(\" \"))\n",
    "        temp.append(d[1].split(\" \"))\n",
    "        dataset.append(temp)\n",
    "    for i in range(len(dataset)):\n",
    "        dataset[i][0] = words2features(dataset[i][0])\n",
    "    return dataset\n",
    "\n",
    "data = generate_dataset(targetUrlList)\n",
    "x_train = [x[0] for x in data]\n",
    "y_train = [x[1] for x in data]\n",
    "\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(x_train[0])\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The POS of a word considered to be a useful feature for labeling. As we don't have the true POS of each word we create the POS ourselfs. We do this by Using The Hebrew POS tagger Of Meni as described [here](https://www.cs.bgu.ac.il/~elhadad/nlpproj/LDAforHebrew.html)\n",
    "\n",
    "Run this block **only if you downloaded and installed Meni's Hebrew Tagger**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (<ipython-input-4-b4b871101d64>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-b4b871101d64>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    sys.path.append('C:\\Users\\Administrator\\Documents\\University\\Semester_C\\NLP\\Project\\lemlda\\tagger')\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "# Right now the call only worked from within the directory lemlda. maybe saving the directory to path will help\n",
    "sys.path.append('C:\\Users\\Administrator\\Documents\\University\\Semester_C\\NLP\\Project\\lemlda\\tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we set our variables the tagger will tag every file in 'corpus' and the output will be in the 'corpus_out' file\n",
    "tag = \"C://Users//Administrator//Documents//University//Semester_C//NLP//Project//lemlda//tagger//tag.bat\"\n",
    "corpus = \"C://Users//Administrator//Documents//University//Semester_C//NLP//Project//Corpus\"\n",
    "corpus_out = \"C://Users//Administrator//Documents//University//Semester_C//NLP//Project//Corpus_out//tags.txt\"\n",
    "\n",
    "subprocess.call([tag,corpus,corpus_out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleaning data - After running this cell, tags will be an array of pairs [word, tag] for every word in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = open(corpus_out, encoding=\"utf-8\").readlines()\n",
    "toke_tagger = [tag.split('\\t') for tag in tagger]\n",
    "tags = [[tag[0].replace(' ',''),tag[1].replace('\\n','')] for tag in toke_tagger if len(tag)>1]\n",
    "\n",
    "print(tags[:10]) # to see the structure of output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third part - Moshe1 take it from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(x_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlList = [\"https://he.wikipedia.org/wiki/%D7%A4%D7%99%D7%9C%D7%95%D7%A1%D7%95%D7%A4%D7%99%D7%94_%D7%A9%D7%9C_%D7%94%D7%9E%D7%93%D7%A2\"]\n",
    "data = generate_dataset(urlList)\n",
    "\n",
    "x_test = [x[0] for x in data]\n",
    "y_test = [x[1] for x in data]\n",
    "y_pred = crf.predict(x_test)\n",
    "print(\"test set size: \" + str(len(x_test)))\n",
    "print(\"---------------------------------------------------\")\n",
    "for i in range(len(y_test[0])):\n",
    "    print(str(i) + \" -  truth: \" + y_test[0][i] + \" prediction: \" + y_pred[0][i])\n",
    "print(\"---------------------------------------------------\")\n",
    "for i in range(len(y_test[1])):\n",
    "    print(str(i) + \" -  truth: \" + y_test[1][i] + \" prediction: \" + y_pred[1][i])\n",
    "print(\"---------------------------------------------------\")\n",
    "for i in range(len(y_test[2])):\n",
    "    print(str(i) + \" -  truth: \" + y_test[2][i] + \" prediction: \" + y_pred[2][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(crf.classes_)\n",
    "labels.remove('none')\n",
    "y_pred = crf.predict(x_test)\n",
    "metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
